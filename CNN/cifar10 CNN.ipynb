{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10 ConvNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Tensorflow version:  2.0.0\n",
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.0 # Set tf2.0 version in Google Colab\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(\"Tensorflow version: \",tf.__version__)\n",
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model with model.fit method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "\n",
    "logdir = 'logs/hparms'\n",
    "chekpoints_dir = os.path.join('chekpoints', \"ckpt_{epoch}\")\n",
    "batch_size = 250 # for having the rest to zero (50000 % 125 = 0)\n",
    "\n",
    "\n",
    "# defining the callbacks\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir, histogram_freq=1, update_freq='epoch', profile_batch = 100000000)  # log metrics\n",
    "#Keras_callback =   # log hparams\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# Save model after every 3 epochs (150000 / 50000)\n",
    "checkpoint_callback = ModelCheckpoint(filepath = chekpoints_dir, save_freq=150000, monitor='val_loss',save_best_only=True, verbose = 1)\n",
    "\n",
    "class TrainCallback(Callback):\n",
    "\n",
    "    def __init__(self, x_train, y_train):\n",
    "        super().__init__()\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(\"<-----------------------------------------------       EPOCH {}     ----------------------------------------------->\".format(epoch+1))\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        (loss, acc) = self.model.evaluate(self.x_train, self.y_train, batch_size = 4096, verbose=0, callbacks=[checkpoint_callback])\n",
    "        print(f\"Real Loss on train : {loss}\")\n",
    "        print(f\"Real Acc on train : {acc}\")\n",
    "    \n",
    "    def on_test_begin(self, epoch, logs=None):\n",
    "        print(\"\\nCalculating the real train loss/accuracy ...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Add, Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def conv_model(hparams):\n",
    "    \n",
    "    dropout_rate = int(hparams[HP_DROPOUT])*0.1\n",
    "    \n",
    "    input = Input((32, 32, 3), name='input')\n",
    "    \n",
    "    conv_1 = Conv2D(96, (5,5), strides=(1, 1), padding='same', activation='relu', name='conv_1')(input)\n",
    "    pool_1 = MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_1')(conv_1)\n",
    "\n",
    "    # adding or not batch normalization\n",
    "    if(hparams[HP_BATCH_NORM] == 'yes'):\n",
    "        pool_1 = BatchNormalization(name='batchNorm_1')(pool_1)\n",
    "    \n",
    "    conv_2 = Conv2D(80, (5,5), strides=(1, 1), padding='same', activation='relu', name='conv_2')(pool_1)\n",
    "    pool_2 = MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_2')(conv_2)\n",
    "    \n",
    "    conv_3 = Conv2D(96, (5,5), strides=(1, 1), padding='same', activation='relu', name='conv_3')(pool_2)\n",
    "    conv_4 = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu', name='conv_4')(conv_3)\n",
    "    conv_5 = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu', name='conv_5')(conv_4)\n",
    "    conv_6 = Conv2D(96, (3, 3), strides=(1, 1), padding='same', activation='relu', name='conv_6')(conv_5)\n",
    "    \n",
    "    flatten = Flatten(name='flatten')(conv_6)\n",
    "    \n",
    "    fc_1 = Dense(120, activation='relu', name='fc_1')(flatten)\n",
    "    fc_2 = Dense(80, activation='relu', name='fc_2')(fc_1)\n",
    "    dropout = Dropout(dropout_rate, name='dropout')(fc_2)\n",
    "    \n",
    "    output = Dense(10, activation='softmax', name='output')(dropout)\n",
    "    model = Model(input, output)\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    if(hparams[HP_OPTIMIZER] == 'Adam'):\n",
    "        model.compile(\n",
    "          optimizer=Adam(learning_rate=float(hparams[HP_LR])),\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'],\n",
    "        )\n",
    "    else:\n",
    "        model.compile(\n",
    "          optimizer=RMSprop(learning_rate=float(hparams[HP_LR])),\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'],\n",
    "        )\n",
    "    \n",
    "    hist = model.fit(x_train, y_train , epochs=30,\n",
    "                     batch_size=batch_size,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     callbacks=[tensorboard_callback, earlyStopping ,TrainCallback(x_train, y_train)])\n",
    "    \n",
    "    return hist.history['val_accuracy'][-1] # returning the last value of validation accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "HP_BATCH_NORM = hp.HParam('batch_norm_active', hp.Discrete(['yes', 'no']))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete(['0', '2', '5']))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['Adam', 'RMSprop']))\n",
    "HP_LR = hp.HParam('learning_rate', hp.Discrete(['0.001', '0.0001']))\n",
    "\n",
    "METRICS = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer(logdir).as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_DROPOUT, HP_OPTIMIZER],\n",
    "        metrics=[hp.Metric(METRICS, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<=========================================       RUN 1      =================================================>\n",
      "{'learning_rate': 0.01, 'batch_norm_active': 'no', 'dropout': 0, 'optimizer': 'Adam'}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv2D)              (None, 32, 32, 96)        7296      \n",
      "_________________________________________________________________\n",
      "pool_1 (MaxPooling2D)        (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 16, 16, 80)        192080    \n",
      "_________________________________________________________________\n",
      "pool_2 (MaxPooling2D)        (None, 8, 8, 80)          0         \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv2D)              (None, 8, 8, 96)          192096    \n",
      "_________________________________________________________________\n",
      "conv_4 (Conv2D)              (None, 8, 8, 64)          55360     \n",
      "_________________________________________________________________\n",
      "conv_5 (Conv2D)              (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv_6 (Conv2D)              (None, 8, 8, 96)          55392     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "fc_1 (Dense)                 (None, 120)               737400    \n",
      "_________________________________________________________________\n",
      "fc_2 (Dense)                 (None, 80)                9680      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                810       \n",
      "=================================================================\n",
      "Total params: 1,287,042\n",
      "Trainable params: 1,287,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "<-----------------------------------------------       EPOCH 1     ----------------------------------------------->\n",
      "Epoch 1/30\n",
      "  750/50000 [..............................] - ETA: 5:40 - loss: 551.0241 - accuracy: 0.1104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0109 16:43:03.492272  9604 callbacks.py:1250] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Loss on train : 2.310355224533081\n",
      "Real Acc on train : 0.10051999986171722\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  750/50000 [..............................] - ETA: 1:09:03 - loss: 551.0241 - accuracy: 0.1104"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2b9bd31d9219>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mrun_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"run-%d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msession_num\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogdir\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'duration : {}min'\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0msession_num\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-2b9bd31d9219>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_dir, hparams)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# record the values used in this trial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMETRICS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-c68f3d4dc7b6>\u001b[0m in \u001b[0;36mconv_model\u001b[1;34m(hparams)\u001b[0m\n\u001b[0;32m     53\u001b[0m                      \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                      \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                      callbacks=[tensorboard_callback, earlyStopping ,TrainCallback(x_train, y_train)])\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# returning the last value of validation accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(train_dir, hparams):\n",
    "    with tf.summary.create_file_writer(train_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = conv_model(hparams)\n",
    "        tf.summary.scalar(METRICS, accuracy, step=1)\n",
    "\n",
    "\n",
    "session_num = 1\n",
    "\n",
    "# varying the optimizers \n",
    "for optimizer in HP_OPTIMIZER.domain.values:\n",
    "    start_time = time.time()\n",
    "    hparams = {\n",
    "        HP_LR: 0.01,\n",
    "        HP_BATCH_NORM: 'no',\n",
    "        HP_DROPOUT: 0,\n",
    "        HP_OPTIMIZER: optimizer,\n",
    "    }\n",
    "    print(\"<=========================================       RUN {}      =================================================>\".format(session_num))\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    train(logdir +\"/\"+ run_name, hparams)\n",
    "    print('duration : {}min' .format(int((start_time - time.time()) / 60)))\n",
    "    session_num += 1  \n",
    "\n",
    "# varying the batch normalization     \n",
    "for bn in HP_BATCH_NORM.domain.values:\n",
    "    start_time = time.time()\n",
    "    hparams = {\n",
    "        HP_LR: 0.01,\n",
    "        HP_BATCH_NORM: bn,\n",
    "        HP_DROPOUT: 0,\n",
    "        HP_OPTIMIZER: 'Adam'\n",
    "    }\n",
    "    print(\"<=========================================       RUN {}      =================================================>\".format(session_num))\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    train(logdir +\"/\"+ run_name, hparams)\n",
    "    print('duration : {}min' .format(int((start_time - time.time()) / 60)))\n",
    "    session_num += 1                \n",
    "                \n",
    "# varying the dropout \n",
    "for dropout_rate in HP_DROPOUT.domain.values:\n",
    "    start_time = time.time()\n",
    "    hparams = {\n",
    "        HP_DROPOUT: dropout_rate,\n",
    "        HP_LR: 0.01,\n",
    "        HP_BATCH_NORM: 'no',\n",
    "        HP_OPTIMIZER: 'Adam'\n",
    "    }\n",
    "    print(\"<=========================================       RUN {}      =================================================>\".format(session_num))\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    train(logdir +\"/\"+ run_name, hparams)\n",
    "    print('duration : {}min' .format(int((start_time - time.time()) / 60)))\n",
    "    session_num += 1     \n",
    "    \n",
    "# varying the learning rate\n",
    "for lr in HP_LR.domain.values:\n",
    "    start_time = time.time()\n",
    "    hparams = {\n",
    "        HP_LR: lr,\n",
    "        HP_BATCH_NORM: 'no',\n",
    "        HP_DROPOUT: 0,\n",
    "        HP_OPTIMIZER: 'Adam',\n",
    "    }\n",
    "    print(\"<=========================================       RUN {}      =================================================>\".format(session_num))\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    train(logdir +\"/\"+ run_name, hparams)\n",
    "    print('duration : {}min' .format(int((start_time - time.time()) / 60)))\n",
    "    session_num += 1        \n",
    "\n",
    "print(\"\\n\\n<=========================================       END      =================================================>\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "img_row, img_height, img_depth = 32,32,3\n",
    "color = True\n",
    "scale = 8\n",
    "\n",
    "model = model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "eval_loss, eval_acc = model.evaluate(eval_dataset)\n",
    "print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\n",
    "\n",
    "def draw_test(name, res, input_im, scale, img_row, img_height):\n",
    "    BLACK = [0,0,0]\n",
    "    res = int(res)\n",
    "    if res == 0:\n",
    "        pred = \"airplane\"\n",
    "    if res == 1:\n",
    "        pred = \"automobile\"\n",
    "    if res == 2:\n",
    "        pred = \"bird\"\n",
    "    if res == 3:\n",
    "        pred = \"cat\"\n",
    "    if res == 4:\n",
    "        pred = \"deer\"\n",
    "    if res == 5:\n",
    "        pred = \"dog\"\n",
    "    if res == 6:\n",
    "        pred = \"frog\"\n",
    "    if res == 7:\n",
    "        pred = \"horse\"\n",
    "    if res == 8:\n",
    "        pred = \"ship\"\n",
    "    if res == 9:\n",
    "        pred = \"truck\"\n",
    "        \n",
    "    expanded_image = cv2.copyMakeBorder(input_im, 0, 0, 0, imageL.shape[0]*2 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    if color == False:\n",
    "        expanded_image = cv2.cvtColor(expanded_image, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.putText(expanded_image, str(pred), (300, 80) , cv2.FONT_HERSHEY_COMPLEX_SMALL,4, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    rand = np.random.randint(0,len(x_test))\n",
    "    input_im = x_test[rand]\n",
    "    imageL = cv2.resize(input_im, None, fx=scale, fy=scale, interpolation = cv2.INTER_CUBIC) \n",
    "    input_im = input_im.reshape(1,img_row, img_height, img_depth) \n",
    "    \n",
    "    ## Get Prediction\n",
    "    res = str(classifier.predict_classes(input_im, 1, verbose = 0)[0])\n",
    "    \n",
    "    draw_test(\"Prediction\", res, imageL, scale, img_row, img_height) \n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model with gradient tape method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0109 17:08:35.139713  9604 deprecation.py:323] From C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 32, 32, 3), (None, 1)), types: (tf.float32, tf.uint8)>\n",
      "Tensorflow version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#x_train = x_train / 255\n",
    "#x_test = x_test / 255\n",
    "\n",
    "\n",
    "#y_train = to_categorical(y_train)\n",
    "#y_test = to_categorical(y_test)\n",
    "\n",
    "def normalize(x, y):\n",
    "    x = tf.image.per_image_standardization(x)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "\n",
    "batch_size = 250\n",
    "train_dataset = train_dataset.shuffle(buffer_size=50000).map(normalize).batch(batch_size)   \n",
    "test_dataset = test_dataset.shuffle(buffer_size=10000).map(normalize).batch(batch_size) \n",
    "\n",
    "print(\"Tensorflow version: \",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Add, Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "\n",
    "def conv_model():\n",
    "    \n",
    "    input = Input((32, 32, 3), name='input')\n",
    "    conv_1 = Conv2D(96, (5,5), strides=(1, 1), padding='same', activation='relu', name='conv_1')(input)\n",
    "    pool_1 = MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_1')(conv_1)\n",
    "\n",
    "    # adding or not batch normalization \n",
    "    #conv_1 = BatchNormalization(name='batchNorm_1')(conv_1)\n",
    "    \n",
    "    conv_2 = Conv2D(80, (5,5), strides=(1, 1), padding='same', activation='relu', name='conv_2')(pool_1)\n",
    "    pool_2 = MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_2')(conv_2)\n",
    "    conv_3 = Conv2D(96, (5,5), strides=(1, 1), padding='same', activation='relu', name='conv_3')(pool_2)\n",
    "\n",
    "    #conv_3 = Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_regularizer=l2(0.001), name='conv_3')(conv_2)\n",
    "    #pool_2 = MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_2')(conv_3)\n",
    "  \n",
    "    conv_4 = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu', name='conv_4')(conv_3)\n",
    "    conv_5 = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu', name='conv_5')(conv_4)\n",
    "    conv_6 = Conv2D(96, (3, 3), strides=(1, 1), padding='same', activation='relu', name='conv_6')(conv_5)\n",
    "    #pool_3 = MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_3')(conv_4)\n",
    "  \n",
    "    #conv_5 = Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_regularizer=l2(0.001), name='conv_5')(pool_3)\n",
    "    #pool_4 = MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_4')(conv_5)\n",
    "  \n",
    "    flatten = Flatten(name='flatten')(conv_6)\n",
    "    fc_1 = Dense(120, activation='relu', name='fc_1')(flatten)\n",
    "    fc_2 = Dense(80, activation='relu', name='fc_2')(fc_1)\n",
    "    dropout_1 = Dropout(0.3, name='dropout_1')(fc_2)\n",
    "    output = Dense(10, activation='softmax', name='output')(dropout_1)\n",
    "    model = Model(input, output)\n",
    "    \n",
    "    model = Model(input, output)\n",
    " \n",
    "    return model\n",
    "\n",
    "model = conv_model()\n",
    "#print(model.summary())\n",
    "#plot_model(model, \"cifar10_CNN.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# set the optimizer\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "\n",
    "class TrainCallback(Callback):\n",
    "\n",
    "    def __init__(self, model, x_train, y_train):\n",
    "        super().__init__()\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.model = model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        (loss, acc) = self.model.evaluate(self.x_train, self.y_train, batch_size = batch_size)\n",
    "        print(f\"Real Loss on train : {loss}\")\n",
    "        print(f\"Real Acc on train : {acc}\")\n",
    "\n",
    "#@tf.function\n",
    "def train_step(model, optimizer, x_train, y_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        predictions = model(x_train, training=True)  # like model.Fit in keras\n",
    "        loss = loss_object(y_train, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    TrainCallback(model, x_train, y_train)\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y_train, predictions)\n",
    "\n",
    "def test_step(model, x_test, y_test):\n",
    "    predictions = model(x_test)\n",
    "    loss = loss_object(y_test, predictions)\n",
    "\n",
    "    test_loss(loss)\n",
    "    test_accuracy(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'cifar10/CNN_' + current_time + '/train/'\n",
    "test_log_dir = 'cifar10/CNN_' + current_time + '/train/'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "model = conv_model()\n",
    "\n",
    "print(\"Start training ...\")\n",
    "for epoch in range(epochs):\n",
    "    print(\"<-----------------------------------------------       EPOCH {}     ----------------------------------------------->\".format(epoch+1))\n",
    "    print(\"Calculating loss/accuracy ...\")\n",
    "    for (x_train, y_train) in train_dataset:\n",
    "        train_step(model, optimizer, x_train, y_train)\n",
    "    # to record data in tensorboard\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "    for (x_test, y_test) in test_dataset:\n",
    "        test_step(model, x_test, y_test)\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "\n",
    "    print('Epoch {}, Loss: {:.3f}, Accuracy: {:.3f}, Test Loss: {:.3f}, Test Accuracy: {:.3f}'.format(epoch+1,train_loss.result(), train_accuracy.result()*100,test_loss.result(), test_accuracy.result()*100))\n",
    "    if((epoch + 1) % 3 == 0):\n",
    "        try:\n",
    "            last_model = 'model_ep'+str(epoch)+'.h5'\n",
    "            model.save(last_model)\n",
    "            print('Saved model to disk ({})', last_model)\n",
    "        except:\n",
    "            print('error occurred when saving model ({})', last_model)\n",
    "\n",
    "print(\"<-----------------------------------------------       END     ----------------------------------------------->\")\n",
    "# Reset metrics every epoch\n",
    "train_loss.reset_states()\n",
    "test_loss.reset_states()\n",
    "train_accuracy.reset_states()\n",
    "test_accuracy.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
