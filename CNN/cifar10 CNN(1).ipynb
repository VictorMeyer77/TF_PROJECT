{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10 ConvNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#x_train = x_train / 255\n",
    "#x_test = x_test / 255\n",
    "\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(\"Tensorflow version: \",tf.__version__)\n",
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model with model.fit method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "\n",
    "logdir = 'logs/hparms'\n",
    "chekpoints_dir = os.path.join('chekpoints', \"ckpt_{epoch}\")\n",
    "batch_size = 125 # for having the rest to zero (50000 % 125 = 0)\n",
    "\n",
    "\n",
    "# defining the callbacks\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir, histogram_freq=1, update_freq='epoch', profile_batch = 100000000)  # log metrics\n",
    "#Keras_callback =   # log hparams\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# Save model after every 3 epochs (150000 / 50000)\n",
    "checkpoint_callback = ModelCheckpoint(filepath = chekpoints_dir, save_freq=150000, monitor='val_loss',save_best_only=True, verbose = 1)\n",
    "\n",
    "class TrainCallback(Callback):\n",
    "\n",
    "    def __init__(self, x_train, y_train):\n",
    "        super().__init__()\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(\"<-----------------------------------------------       EPOCH {}     ----------------------------------------------->\".format(epoch+1))\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        (loss, acc) = self.model.evaluate(self.x_train, self.y_train, batch_size = 8192, verbose=0, callbacks=[checkpoint_callback])\n",
    "        print(f\"Real Loss on train : {loss}\")\n",
    "        print(f\"Real Acc on train : {acc}\")\n",
    "    \n",
    "    def on_test_begin(self, epoch, logs=None):\n",
    "        print(\"\\nCalculating the real train loss/accuracy ...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Add, Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def conv_model(hparams):\n",
    "    \n",
    "    dropout_rate = int(hparams[HP_DROPOUT])*0.1\n",
    "    \n",
    "    input = Input((32, 32, 3), name='input')\n",
    "    \n",
    "    # adding or not batch normalization layer\n",
    "    if(hparams[HP_BATCH_NORM] == 'yes'):\n",
    "        batchNorm_1 = BatchNormalization(name='batchNorm_1')(input)\n",
    "        conv_1 = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu',kernel_regularizer=l2(0.001), name='conv_1')(batchNorm_1)\n",
    "    else:\n",
    "        conv_1 = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu',kernel_regularizer=l2(0.001), name='conv_1')(input)\n",
    "    \n",
    "    pool_1 = MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_1')(conv_1)\n",
    "    \n",
    "    conv_2 = Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_regularizer=l2(0.001), name='conv_2')(pool_1)\n",
    "    pool_2 = MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_2')(conv_2)\n",
    "    \n",
    "    flatten = Flatten(name='flatten')(pool_2)\n",
    "    fc_1 = Dense(256, activation='linear', name='fc_1')(flatten)\n",
    "    dropout_1 = Dropout(dropout_rate, name='dropout_1')(fc_1)\n",
    "    relu_1 = Activation('relu', name='relu_1')(dropout_1)\n",
    "    fc_2 = Dense(256, activation='relu', name='fc_2')(relu_1)\n",
    "    dropout_2 = Dropout(dropout_rate, name='dropout_2')(fc_2)\n",
    "    relu_2 = Activation('relu', name='relu_2')(dropout_2)\n",
    "    output = Dense(10, activation='softmax', name='output')(relu_2)\n",
    "    model = Model(input, output)\n",
    "    \n",
    "    if(hparams[HP_OPTIMIZER] == 'Adam'):\n",
    "        model.compile(\n",
    "          optimizer=Adam(learning_rate=float(hparams[HP_LR])),\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'],\n",
    "        )\n",
    "    else:\n",
    "        model.compile(\n",
    "          optimizer=RMSprop(learning_rate=float(hparams[HP_LR])),\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'],\n",
    "        )\n",
    "    \n",
    "    hist = model.fit(x_train, y_train , epochs=30,\n",
    "                     batch_size=batch_size,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     callbacks=[tensorboard_callback, earlyStopping ,TrainCallback(x_train, y_train)])\n",
    "    \n",
    "    return hist.history['val_accuracy'][-1] # returning the last value of validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "HP_BATCH_NORM = hp.HParam('batch_norm_active', hp.Discrete(['yes', 'no']))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete(['0', '2', '5']))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['Adam', 'RMSprop']))\n",
    "HP_LR = hp.HParam('learning_rate', hp.Discrete(['0.01', '0.001']))\n",
    "\n",
    "\n",
    "METRICS = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer(logdir).as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_DROPOUT, HP_OPTIMIZER],\n",
    "        metrics=[hp.Metric(METRICS, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(train_dir, hparams):\n",
    "    with tf.summary.create_file_writer(train_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = conv_model(hparams)\n",
    "        tf.summary.scalar(METRICS, accuracy, step=1)\n",
    "\n",
    "\n",
    "session_num = 1\n",
    "\n",
    "# varying the optimizers \n",
    "for optimizer in HP_OPTIMIZER.domain.values:\n",
    "    start_time = time.time()\n",
    "    hparams = {\n",
    "        HP_LR: 0.01,\n",
    "        HP_BATCH_NORM: 'no',\n",
    "        HP_DROPOUT: 0,\n",
    "        HP_OPTIMIZER: optimizer,\n",
    "    }\n",
    "    print(\"<=========================================       RUN {}      =================================================>\".format(session_num))\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    train(logdir +\"/\"+ run_name, hparams)\n",
    "    print('duration : {}min' .format(int((start_time - time.time()) / 60)))\n",
    "    session_num += 1  \n",
    "\n",
    "# varying the batch normalization     \n",
    "for bn in HP_BATCH_NORM.domain.values:\n",
    "    start_time = time.time()\n",
    "    hparams = {\n",
    "        HP_LR: 0.01,\n",
    "        HP_BATCH_NORM: bn,\n",
    "        HP_DROPOUT: 0,\n",
    "        HP_OPTIMIZER: 'Adam'\n",
    "    }\n",
    "    print(\"<=========================================       RUN {}      =================================================>\".format(session_num))\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    train(logdir +\"/\"+ run_name, hparams)\n",
    "    print('duration : {}min' .format(int((start_time - time.time()) / 60)))\n",
    "    session_num += 1                \n",
    "                \n",
    "# varying the dropout \n",
    "for dropout_rate in HP_DROPOUT.domain.values:\n",
    "    start_time = time.time()\n",
    "    hparams = {\n",
    "        HP_DROPOUT: dropout_rate,\n",
    "        HP_LR: 0.01,\n",
    "        HP_BATCH_NORM: 'no',\n",
    "        HP_OPTIMIZER: 'Adam'\n",
    "    }\n",
    "    print(\"<=========================================       RUN {}      =================================================>\".format(session_num))\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    train(logdir +\"/\"+ run_name, hparams)\n",
    "    print('duration : {}min' .format(int((start_time - time.time()) / 60)))\n",
    "    session_num += 1     \n",
    "    \n",
    "# varying the learning rate\n",
    "for lr in HP_LR.domain.values:\n",
    "    start_time = time.time()\n",
    "    hparams = {\n",
    "        HP_LR: lr,\n",
    "        HP_BATCH_NORM: 'no',\n",
    "        HP_DROPOUT: 0,\n",
    "        HP_OPTIMIZER: 'Adam',\n",
    "    }\n",
    "    print(\"<=========================================       RUN {}      =================================================>\".format(session_num))\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    train(logdir +\"/\"+ run_name, hparams)\n",
    "    print('duration : {}min' .format(int((start_time - time.time()) / 60)))\n",
    "    session_num += 1        \n",
    "\n",
    "print(\"\\n\\n<=========================================       END      =================================================>\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN FIN FIN FIN FIN FIN FIN FIN FIN FIN FIN FIN FIN FIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "eval_loss, eval_acc = model.evaluate(eval_dataset)\n",
    "\n",
    "print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model with gradient tape method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(x, y):\n",
    "    x = tf.image.per_image_standardization(x)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=50000).map(normalize).batch(batch_size)   \n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "    add image representation for model from plt.model_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#x_train = x_train / 255\n",
    "#x_test = x_test / 255\n",
    "\n",
    "\n",
    "#y_train = to_categorical(y_train)\n",
    "#y_test = to_categorical(y_test)\n",
    "\n",
    "def normalize(x, y):\n",
    "    x = tf.image.per_image_standardization(x)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "\n",
    "batch_size = 250\n",
    "train_dataset = train_dataset.shuffle(buffer_size=50000).map(normalize).batch(batch_size)   \n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "print(\"Tensorflow version: \",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Add, Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "\n",
    "def conv_model():\n",
    "    \n",
    "    input = Input((32, 32, 3), name='input')\n",
    "\n",
    "    conv_1 = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu', name='conv_1', input_shape = (32, 32, 3), data_format=\"channels_last\")\n",
    "    pool_1 = MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_1')(conv_1)\n",
    "    \n",
    "    conv_2 = Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu', name='conv_2')(pool_1)\n",
    "    pool_2 = MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_2')(conv_2)\n",
    "    \n",
    "    conv_3 = Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', name='conv_3')(pool_2)\n",
    "    pool_3 = MaxPooling2D(pool_size=(2, 2), padding='same', name='pool_3')(conv_3)\n",
    "    \n",
    "    flatten = Flatten(name='flatten')(pool_3)\n",
    "    fc_1 = Dense(512, activation='relu', name='fc_1')(flatten)\n",
    "    fc_2 = Dense(256, activation='relu', name='fc_2')(fc_1)\n",
    "    fc_3 = Dense(128, activation='relu', name='fc_3')(fc_2)\n",
    "    output = Dense(10, activation='softmax', name='output')(fc_3)\n",
    "    \n",
    "    model = Model(conv_1, output)\n",
    " \n",
    "    return model\n",
    "\n",
    "model = conv_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(model, \"cifar10_CNN.png\")\n",
    "# set metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# set the optimizer\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "\n",
    "class TrainCallback(Callback):\n",
    "\n",
    "    def __init__(self, model, x_train, y_train):\n",
    "        super().__init__()\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.model = model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        (loss, acc) = self.model.evaluate(self.x_train, self.y_train, batch_size = batch_size)\n",
    "        print(f\"Real Loss on train : {loss}\")\n",
    "        print(f\"Real Acc on train : {acc}\")\n",
    "\n",
    "#@tf.function\n",
    "def train_step(model, optimizer, x_train, y_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        predictions = model(x_train, training=True)  # like model.Fit in keras\n",
    "        loss = loss_object(y_train, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    TrainCallback(model, x_train, y_train)\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y_train, predictions)\n",
    "\n",
    "def test_step(model, x_test, y_test):\n",
    "    predictions = model(x_test)\n",
    "    loss = loss_object(y_test, predictions)\n",
    "\n",
    "    test_loss(loss)\n",
    "    test_accuracy(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'cifar10/CNN_' + current_time + '/train/'\n",
    "test_log_dir = 'cifar10/CNN_' + current_time + '/train/'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "model = conv_model()\n",
    "\n",
    "print(\"Start training ...\")\n",
    "for epoch in range(epochs):\n",
    "    print(\"<-----------------------------------------------       EPOCH {}     ----------------------------------------------->\".format(epoch+1))\n",
    "    print(\"Calculating loss/accuracy ...\")\n",
    "    for (x_train, y_train) in train_dataset:\n",
    "        train_step(model, optimizer, x_train, y_train)\n",
    "    # to record data in tensorboard\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "    for (x_test, y_test) in test_dataset:\n",
    "        test_step(model, x_test, y_test)\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "\n",
    "    print('Epoch {}, Loss: {:.3f}, Accuracy: {:.3f}, Test Loss: {:.3f}, Test Accuracy: {:.3f}'.format(epoch+1,train_loss.result(), train_accuracy.result()*100,test_loss.result(), test_accuracy.result()*100))\n",
    "    if((epoch + 1) % 3 == 0):\n",
    "        try:\n",
    "            last_model = 'model_ep'+str(epoch)+'.h5'\n",
    "            model.save(last_model)\n",
    "            print('Saved model to disk ({})', last_model)\n",
    "        except:\n",
    "            print('error occurred when saving model ({})', last_model)\n",
    "\n",
    "print(\"<-----------------------------------------------       END     ----------------------------------------------->\")\n",
    "# Reset metrics every epoch\n",
    "train_loss.reset_states()\n",
    "test_loss.reset_states()\n",
    "train_accuracy.reset_states()\n",
    "test_accuracy.reset_states()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir 'cifar10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(last_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "img_row, img_height, img_depth = 32,32,3\n",
    "classifier = load_model(\"C:/Users/Asus/DeepLearningCV/Trained Models/cifar_simple_cnn.h5\")\n",
    "color = True\n",
    "scale = 8\n",
    "\n",
    "def draw_test(name, res, input_im, scale, img_row, img_height):\n",
    "    BLACK = [0,0,0]\n",
    "    res = int(res)\n",
    "    if res == 0:\n",
    "        pred = \"airplane\"\n",
    "    if res == 1:\n",
    "        pred = \"automobile\"\n",
    "    if res == 2:\n",
    "        pred = \"bird\"\n",
    "    if res == 3:\n",
    "        pred = \"cat\"\n",
    "    if res == 4:\n",
    "        pred = \"deer\"\n",
    "    if res == 5:\n",
    "        pred = \"dog\"\n",
    "    if res == 6:\n",
    "        pred = \"frog\"\n",
    "    if res == 7:\n",
    "        pred = \"horse\"\n",
    "    if res == 8:\n",
    "        pred = \"ship\"\n",
    "    if res == 9:\n",
    "        pred = \"truck\"\n",
    "        \n",
    "    expanded_image = cv2.copyMakeBorder(input_im, 0, 0, 0, imageL.shape[0]*2 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    if color == False:\n",
    "        expanded_image = cv2.cvtColor(expanded_image, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.putText(expanded_image, str(pred), (300, 80) , cv2.FONT_HERSHEY_COMPLEX_SMALL,4, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    rand = np.random.randint(0,len(x_test))\n",
    "    input_im = x_test[rand]\n",
    "    imageL = cv2.resize(input_im, None, fx=scale, fy=scale, interpolation = cv2.INTER_CUBIC) \n",
    "    input_im = input_im.reshape(1,img_row, img_height, img_depth) \n",
    "    \n",
    "    ## Get Prediction\n",
    "    res = str(classifier.predict_classes(input_im, 1, verbose = 0)[0])\n",
    "    \n",
    "    draw_test(\"Prediction\", res, imageL, scale, img_row, img_height) \n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 150\n",
    "\n",
    "@tf.function  # graph mode\n",
    "def train_(batch_images, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        predictions = model(batch_images)\n",
    "        loss = loss_object(targets, predictions) # get loss of these predictions\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # to join each gradient\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets, predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "input = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input')\n",
    "output =  tf.placeholder(tf.float32, shape=(None, 10), name='output')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "         batch_size = 64,\n",
    "         epochs = 150,\n",
    "         shuffle = True,\n",
    "         validation_data = (x_test,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = Conv2D(64,(3,3), strides=(1,1), padding='same', activation='relu',name='conv1')(input)\n",
    "    conv1 = Conv2D(64,(3,3), strides=(1,1), padding='same', activation='relu',name='conv1')(input)\n",
    "    \n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', input_shape=x_train.shape[1:]))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
